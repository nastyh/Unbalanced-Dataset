{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\us61565\\Desktop\\Explainability\\Framingham\\framingham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at stats\n",
    "# pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the target variable\n",
    "df['TenYearCHD'].value_counts(normalize = True)\n",
    "sns.countplot(x='TenYearCHD',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring cigsPerDay\n",
    "df['cigsPerDay'].value_counts(normalize = True).plot(kind=\"bar\")\n",
    "df['cigsPerDay'][df['currentSmoker']==0].isna().sum() # Are there any NaNs for non-smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a boolean array of smokers\n",
    "smoke = (df['currentSmoker']==1)\n",
    "# applying mean to NaNs in cigsPerDay but using a set of smokers only\n",
    "df.loc[smoke,'cigsPerDay'] = df.loc[smoke,'cigsPerDay'].fillna(df.loc[smoke,'cigsPerDay'].mean())\n",
    "df['cigsPerDay'][df['currentSmoker']==1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling out missing values\n",
    "df['BPMeds'].fillna(0, inplace = True)\n",
    "df['glucose'].fillna(df.glucose.mean(), inplace = True)\n",
    "df['totChol'].fillna(df.totChol.mean(), inplace = True)\n",
    "df['education'].fillna(1, inplace = True)\n",
    "df['BMI'].fillna(df.BMI.mean(), inplace = True)\n",
    "df['heartRate'].fillna(df.heartRate.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # are ther any NaNs left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nice thing to have in front of the eyes: all histograms together\n",
    "def draw_histograms(dataframe, features, rows, cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax=fig.add_subplot(rows,cols,i+1)\n",
    "        dataframe[feature].hist(bins=15,ax=ax,facecolor='midnightblue')\n",
    "        ax.set_title(feature+\" Distribution\",color='DarkRed')\n",
    "        \n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "draw_histograms(df,df.columns,6,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:,:-1]\n",
    "result = df.iloc[:,-1] # the last column is what we are about to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, result, test_size = 0.2, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what features are the most important?\n",
    "plt.plot(rf.feature_importances_)\n",
    "plt.xticks(np.arange(X_train.shape[1]), X_train.columns.tolist(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a list of the features and their importance scores\n",
    "list(zip(features, rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on unseen data\n",
    "predictions_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under ROC curve\n",
    "prob_rf = rf.predict_proba(X_test)\n",
    "prob_rf = [p[1] for p in prob_rf]\n",
    "print(roc_auc_score(y_test, prob_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.12\n",
    "sfm = SelectFromModel(clf, threshold=0.12)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = list(features.columns.values) # creating a list with features' names\n",
    "for feature_list_index in sfm.get_support(indices=True):\n",
    "    print(feat_labels[feature_list_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with only imporant features. Can check X_important_train.shape[1]\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n",
    "clf_important.fit(X_important_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_y_4 = clf_important.predict(X_important_test)\n",
    "print(classification_report(y_test, predictions_y_4))\n",
    "print(confusion_matrix(y_test, predictions_y_4))\n",
    "accuracy_score(y_test, predictions_y_4)\n",
    "# Under ROC curve\n",
    "prob_y_4 = clf_important.predict_proba(X_important_test)\n",
    "prob_y_4 = [p[1] for p in prob_y_4]\n",
    "print(roc_auc_score(y_test, prob_y_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression(solver='liblinear')\n",
    "logmodel.fit(X_train_std, y_train)\n",
    "predictions_y_2 = logmodel.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions_y_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions_y_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions_y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under ROC curve\n",
    "prob_y_2 = logmodel.predict_proba(X_test_std)\n",
    "prob_y_2 = [p[1] for p in prob_y_2]\n",
    "print(roc_auc_score(y_test, prob_y_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "logmodel.fit(X_train_std, y_train)\n",
    "predictions_y_3 = logmodel.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fewer Type II errors but less accurate \n",
    "print(classification_report(y_test, predictions_y_3))\n",
    "print(confusion_matrix(y_test, predictions_y_3))\n",
    "accuracy_score(y_test, predictions_y_3)\n",
    "# Under ROC curve\n",
    "prob_y_3 = logmodel.predict_proba(X_test_std)\n",
    "prob_y_3 = [p[1] for p in prob_y_3]\n",
    "print(roc_auc_score(y_test, prob_y_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "weights = np.linspace(0.03, 0.97, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch should be done on the big dataset, before it's split in train and test. Thus, normalizing features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc = GridSearchCV(\n",
    "    estimator=LogisticRegression(solver='liblinear'),\n",
    "    param_grid={\n",
    "        'class_weight': [{0: x, 1: 1.0-x} for x in weights]\n",
    "    },\n",
    "    scoring='roc_auc',\n",
    "    cv=3\n",
    ")\n",
    "grid_result = gsc.fit(features_std, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters : %s\" % grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights vs f1 score\n",
    "dataz = pd.DataFrame({ 'score': grid_result.cv_results_['mean_test_score'],\n",
    "                       'weight': weights })\n",
    "dataz.plot(x='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing weights found above\n",
    "rf_w = RandomForestClassifier(class_weight = {0:0.882962962962963, 1:0.11703703703703705})\n",
    "rf_w.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on unseen data\n",
    "predictions_rf_w = rf_w.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit worse than with default parameters\n",
    "print(classification_report(y_test, predictions_rf_w))\n",
    "print(confusion_matrix(y_test, predictions_rf_w))\n",
    "accuracy_score(y_test, predictions_rf_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or to the logistic regression:\n",
    "weights = {0 : '0.882962962962963', 1 : '0.11703703703703705'}\n",
    "logmodel_auto_gridsearch = LogisticRegression(class_weight = weights, solver = 'liblinear')\n",
    "logmodel_auto_gridsearch.fit(X_train_std, y_train)\n",
    "predictions_std_auto_gridsearch = logmodel_auto_gridsearch.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions_std_auto_gridsearch))\n",
    "print(confusion_matrix(y_test, predictions_std_auto_gridsearch))\n",
    "accuracy_score(y_test, predictions_std_auto_gridsearch)\n",
    "# Under ROC curve\n",
    "prob_y_3_gridsearch = logmodel_auto_gridsearch.predict_proba(X_test_std)\n",
    "prob_y_3_gridsearch= [p[1] for p in prob_y_3_gridsearch]\n",
    "print(roc_auc_score(y_test, prob_y_3_gridsearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling/downsampling manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority = df[df.TenYearCHD==1]\n",
    "df_majority = df[df.TenYearCHD==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TenYearCHD'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority class\n",
    "# sample with replacement to match majority class and get reproducible results\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=3596,    \n",
    "                                 random_state=123) \n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.TenYearCHD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test, normalize the new data set\n",
    "features_upsampled = df_upsampled.iloc[:,:-1]\n",
    "result_upsampled = df_upsampled.iloc[:,-1]\n",
    "\n",
    "X_train_upsampled, X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(features_upsampled, result_upsampled, test_size = 0.2, random_state = 14)\n",
    "\n",
    "X_train_std_upsampled = scaler.fit_transform(X_train_upsampled)\n",
    "X_test_std_upsampled = scaler.fit_transform(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new log model for upsampled data\n",
    "logmodel_upsampled = LogisticRegression(solver='liblinear')\n",
    "logmodel_upsampled.fit(X_train_std_upsampled, y_train_upsampled)\n",
    "predictions_y_2_upsampled = logmodel_upsampled.predict(X_test_std_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very poor results\n",
    "print(classification_report(y_test_upsampled, predictions_y_2_upsampled))\n",
    "print(confusion_matrix(y_test_upsampled, predictions_y_2_upsampled))\n",
    "accuracy_score(y_test_upsampled, predictions_y_2_upsampled)\n",
    "# Under ROC curve\n",
    "prob_y_2_upsampled = logmodel_upsampled.predict_proba(X_test_std_upsampled)\n",
    "prob_y_2_upsampled = [p[1] for p in prob_y_2_upsampled]\n",
    "print(roc_auc_score(y_test_upsampled, prob_y_2_upsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowering the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel_lowering = LogisticRegression(solver='liblinear')\n",
    "logmodel_lowering.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a list of the features and their importance scores\n",
    "list(zip(features, clf_important.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(1,7):\n",
    "    cm2=0\n",
    "    predictions_y_2_lowering = logmodel_lowering.predict_proba(X_test_std)\n",
    "    y_pred2_lowering=binarize(predictions_y_2_lowering,i/10)[:,1]\n",
    "    cm2=confusion_matrix(y_test,y_pred2_lowering)\n",
    "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n",
    "            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n",
    "          'Sensitivity: ',cm2[1,1]/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X_train_std,label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.05,\n",
    "                max_depth = 8, alpha = 10, n_estimators = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xg_reg.fit(X_train_std,y_train)\n",
    "eval_set = [(X_test_std, y_test)]\n",
    "xg_reg.fit(X_train_std, y_train, eval_metric=\"error\", eval_set = eval_set, verbose = True)\n",
    "prediction_y_5 = xg_reg.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, prediction_y_5))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do feature importance using XGBoost, as well \n",
    "list(zip(features.columns, xg_reg.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 20, 30, 40, 50, 60]\n",
    "max_depth = [2, 4, 5, 6, 7, 8]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(max_depth = max_depth, n_estimators = n_estimators, learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "grid_search_xg = GridSearchCV(xg_reg, param_grid, scoring = 'roc_auc', n_jobs = -1, cv=kfold, verbose = 1)\n",
    "result_gcv_xgb = grid_search_xg.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: %f using %s\" % (result_gcv_xgb.best_score_, result_gcv_xgb.best_params_))\n",
    "means = result_gcv_xgb.cv_results_['mean_test_score']\n",
    "stds = result_gcv_xgb.cv_results_['std_test_score']\n",
    "params = result_gcv_xgb.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild using best params\n",
    "xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 2, alpha = 10, n_estimators = 50)\n",
    "eval_set = [(X_test_std, y_test)]\n",
    "xg_reg.fit(X_train_std, y_train, eval_metric=\"error\", eval_set = eval_set, verbose = False)\n",
    "prediction_y_5 = xg_reg.predict(X_test_std)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, prediction_y_5))\n",
    "roc = roc_auc_score(y_test, prediction_y_5, average = 'weighted')\n",
    "print(\"AUC: {:.2%}\".format(roc))\n",
    "print(\"RMSE: {:.3}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_y_5_01 = prediction_y_5\n",
    "prediction_y_5_01[prediction_y_5 > 0.5] = 1\n",
    "prediction_y_5_01[prediction_y_5 <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediction_y_5_01))\n",
    "print(confusion_matrix(y_test, prediction_y_5_01))\n",
    "accuracy_score(y_test, prediction_y_5_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost on the upsampled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 20, 30, 40, 50, 60]\n",
    "max_depth = [2, 4, 5, 6, 7, 8]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(max_depth = max_depth, n_estimators = n_estimators, learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "grid_search_xg = GridSearchCV(xg_reg, param_grid, scoring = 'roc_auc', n_jobs = -1, cv=kfold, verbose = 1)\n",
    "result_gcv_xgb = grid_search_xg.fit(X_train_std_upsampled, y_train_upsampled)\n",
    "\n",
    "print(\"Best score: %f using %s\" % (result_gcv_xgb.best_score_, result_gcv_xgb.best_params_))\n",
    "means = result_gcv_xgb.cv_results_['mean_test_score']\n",
    "stds = result_gcv_xgb.cv_results_['std_test_score']\n",
    "params = result_gcv_xgb.cv_results_['params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild using best params\n",
    "xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.3,\n",
    "                max_depth = 8, alpha = 10, n_estimators = 60)\n",
    "eval_set = [(X_test_std_upsampled, y_test_upsampled)]\n",
    "xg_reg.fit(X_train_std_upsampled, y_train_upsampled, eval_metric=[\"auc\",\"error\"], eval_set = eval_set, verbose = False)\n",
    "prediction_y_5 = xg_reg.predict(X_test_std_upsampled)\n",
    "roc = roc_auc_score(y_test_upsampled, prediction_y_5, average = 'weighted')\n",
    "rmse = np.sqrt(mean_squared_error(y_test_upsampled, prediction_y_5))\n",
    "print(\"AUC: {:.2%}\".format(roc))\n",
    "print(\"RMSE: {:.3}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(xg_reg, X_train_std, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (-scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Learning curve'\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "plot_learning_curve(xg_reg, title, X_train_std, y_train, cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.plot_tree(xg_reg,num_trees=0)\n",
    "# plt.rcParams['figure.figsize'] = [50, 10]\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KNeighborsClassifier(n_neighbors=3)\n",
    "bag = BaggingClassifier(\n",
    "    m, \n",
    "    max_samples=.5, \n",
    "    max_features=6, \n",
    "    n_jobs=5,\n",
    "    oob_score=True)\n",
    "bag.fit(X_train_std, y_train)\n",
    "\n",
    "bag.oob_score_\n",
    "bag.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab = AdaBoostClassifier(base_estimator=None, n_estimators=100)\n",
    "adab.fit(X_train_std, y_train)\n",
    "adab.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = VotingClassifier(\n",
    "    estimators=[('Kneigh', KNeighborsClassifier()), \n",
    "                ('AdaBoost', AdaBoostClassifier()), \n",
    "                ('RandomForest', RandomForestClassifier())], \n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest Classifier from above\n",
    "rf = RandomForestClassifier()\n",
    "# rf.fit(X_train, y_train)\n",
    "calibrated = CalibratedClassifierCV(rf, method='sigmoid', cv=5)\n",
    "calibrated.fit(X_train, y_train)\n",
    "predictions_y_6_calibrated = calibrated.predict_proba(X_test)[:, 1]\n",
    "fop, mpv = calibration_curve(y_test, predictions_y_6_calibrated, n_bins=10, normalize=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--', label = 'Perfectly Calibrated')\n",
    "pyplot.plot(mpv, marker='.', label = 'uncalibrated')\n",
    "pyplot.plot(fop, marker = 'v', label = 'calibrated')\n",
    "pyplot.legend(loc = 'upper left')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrated and uncalibrated together\n",
    "def uncalibrated(X_train, X_test, y_train):\n",
    "    rf = SVC()\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf.decision_function(X_test)\n",
    "\n",
    "def calibrated(X_train, X_test, y_train):\n",
    "    rf = SVC()\n",
    "    calibrated = CalibratedClassifierCV(rf, method='sigmoid', cv=5)\n",
    "    calibrated.fit(X_train, y_train)\n",
    "    return calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "yhat_uncalibrated = uncalibrated(X_train, X_test, y_train)\n",
    "yhat_calibrated = calibrated(X_train, X_test, y_train)\n",
    "\n",
    "fop_uncalibrated, mpv_uncalibrated = calibration_curve(y_test, yhat_uncalibrated, n_bins =6, normalize=True)\n",
    "fop_calibrated, mpv_calibrated = calibration_curve(y_test, yhat_calibrated, n_bins =6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--', color='black', label = 'normal')\n",
    "pyplot.plot(mpv_uncalibrated, label = 'uncalibrated', marker = 'v')\n",
    "pyplot.plot(fop_uncalibrated, label = 'calibrated', marker = '.')\n",
    "pyplot.xlabel(\"Predicted probabilities' Frequency\")\n",
    "pyplot.ylabel(\"What was observed\")\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
